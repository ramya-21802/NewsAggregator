# -*- coding: utf-8 -*-
"""NewsAggreaggator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1de785MzId2Rer6qTdbWd9xsmEwkyCnPm
"""

pip install requests beautifulsoup4 selenium pandas

!pip install selenium beautifulsoup4 requests pandas

!apt-get update
!apt install -y chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin

import sys
sys.path.insert(0, '/usr/lib/chromium-browser/chromedriver')

import requests
from bs4 import BeautifulSoup
import csv
import datetime
import pandas as pd

# Function to fetch and parse articles
def fetch_news(url, source_name):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    articles = soup.find_all('div', class_='article')  # Update based on site structure

    news_data = []

    for article in articles:
        title = article.find('h2').text.strip()
        summary = article.find('p').text.strip()
        pub_date = datetime.datetime.now().strftime('%Y-%m-%d')  # Example date handling
        article_url = article.find('a')['href']

        news_data.append([title, summary, pub_date, source_name, article_url])

    return news_data

news_sources = [
    {'url': 'https://www.bbc.com/news', 'name': 'BBC'},
    {'url': 'https://edition.cnn.com/world', 'name': 'CNN'}
]

all_news = []
for source in news_sources:
    news = fetch_news(source['url'], source['name'])
    all_news.extend(news)

# Save to CSV in Colab
df = pd.DataFrame(all_news, columns=['Title', 'Summary', 'Publication Date', 'Source', 'URL'])
df.to_csv('news_articles.csv', index=False)

from google.colab import files
files.download('news_articles.csv')

!pip install spacy
!python -m spacy download en_core_web_sm

import spacy
import pandas as pd

nlp = spacy.load('en_core_web_sm')

# Load the CSV file
df = pd.read_csv('news_articles.csv')

# Categorization function based on keywords
def categorize_article(text):
    doc = nlp(text)
    if 'politics' in text.lower():
        return 'Politics'
    elif 'technology' in text.lower():
        return 'Technology'
    elif 'sports' in text.lower():
        return 'Sports'
    else:
        return 'Other'

# Apply categorization to each article summary
df['Category'] = df['Summary'].apply(categorize_article)

# Save updated CSV
df.to_csv('news_articles.csv', index=False)

# Download the updated CSV
files.download('news_articles.csv')

!pip install fastapi uvicorn pandas

!pip install nest_asyncio

import nest_asyncio
nest_asyncio.apply()

from fastapi import FastAPI
import pandas as pd
import uvicorn
import nest_asyncio

nest_asyncio.apply()

app = FastAPI()

# Load the CSV file
df = pd.read_csv('news_articles.csv')

# Endpoint to get all articles
@app.get("/articles")
def get_articles(category: str = None):
    if category:
        filtered_articles = df[df['Category'].str.contains(category, case=False, na=False)]
        return filtered_articles.to_dict(orient='records')
    return df.to_dict(orient='records')

# Endpoint to get a specific article by index (ID)
@app.get("/articles/{article_id}")
def get_article(article_id: int):
    if 0 <= article_id < len(df):
        return df.iloc[article_id].to_dict()
    return {"error": "Article not found"}

# Endpoint to search articles by keyword
@app.get("/search")
def search_articles(keyword: str):
    filtered_articles = df[df['Summary'].str.contains(keyword, case=False, na=False)]
    return filtered_articles.to_dict(orient='records')

# Run the FastAPI app in Colab
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)